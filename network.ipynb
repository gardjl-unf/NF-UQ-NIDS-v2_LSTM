{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6154ffee-09da-463c-a9e3-38b57f3c4794",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow\n",
    "!pip install numpy\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39dc9843-b7c2-4b6e-a960-7aaf2fe15371",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-20 21:09:03.943794: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-20 21:09:04.716728: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-20 21:09:05,301 — __main__ — __init__:237 — Logging initialized -- __main__\n",
      "2024-04-20 21:09:05,302 — __main__ — load_data:52 — Loading data from /data/NF-UQ-NIDS-v2.csv\n",
      "2024-04-20 21:11:48,870 — __main__ — _process_data:58 — Processing data\n",
      "2024-04-20 21:11:48,870 — __main__ — _process_data:59 — Converting IP addresses to integers\n",
      "2024-04-20 21:13:15,546 — __main__ — _process_data:62 — Dropping Attack and Dataset columns\n",
      "2024-04-20 21:13:35,380 — __main__ — _normalize_data:118 — Normalizing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda/lib/python3.9/site-packages/pandas/core/nanops.py:1016: RuntimeWarning: overflow encountered in square\n",
      "  sqr = _ensure_numeric((avg - values) ** 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-20 21:13:54,082 — __main__ — _normalize_data:118 — Normalizing data\n",
      "2024-04-20 21:14:17,254 — __main__ — _split_data:94 — Reshaped train_x shape: (607904, 100, 43)\n",
      "2024-04-20 21:14:17,255 — __main__ — _split_data:95 — Reshaped train_y shape: (607904, 100, 1)\n",
      "2024-04-20 21:14:17,255 — __main__ — _process_data:66 — Data dimensions: (75987976, 44)\n",
      "2024-04-20 21:14:17,256 — __main__ — _process_data:67 — Training data dimensions: (607904, 100, 43)\n",
      "2024-04-20 21:14:17,256 — __main__ — _process_data:68 — Testing data dimensions: (607904, 100, 43)\n",
      "2024-04-20 21:14:17,435 — __main__ — __init__:145 — No model UUID provided. Generating new UUID\n",
      "2024-04-20 21:14:17,436 — __main__ — __init__:148 — New UUID: 2f50e09c-e5fa-4c13-8592-74a2c816937e\n",
      "2024-04-20 21:14:17,436 — __main__ — __init__:149 — Initializing new model\n",
      "2024-04-20 21:14:17,437 — __main__ — init_model:153 — Initializing model\n",
      "2024-04-20 21:14:17,498 — __main__ — init_model:172 — Model initialized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-20 21:14:17.381460: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-20 21:14:17.381799: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-20 21:14:17.382076: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-20 21:14:17.382349: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-20 21:14:17.416128: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2251] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: (607904, 100, 43)\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-20 21:14:20.615201: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 10455948800 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18995/18997\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.6689 - loss: 0.1102"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-20 21:19:28.761036: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 10455948800 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18997/18997\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m426s\u001b[0m 22ms/step - accuracy: 0.6689 - loss: 0.1102 - val_accuracy: 0.6688 - val_loss: 0.1100\n",
      "Epoch 2/10\n",
      "\u001b[1m18997/18997\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m415s\u001b[0m 22ms/step - accuracy: 0.6688 - loss: 0.1100 - val_accuracy: 0.6688 - val_loss: 0.1100\n",
      "Epoch 3/10\n",
      "\u001b[1m18997/18997\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m415s\u001b[0m 22ms/step - accuracy: 0.6689 - loss: 0.1099 - val_accuracy: 0.6688 - val_loss: 0.1099\n",
      "Epoch 4/10\n",
      "\u001b[1m18997/18997\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m415s\u001b[0m 22ms/step - accuracy: 0.6689 - loss: 0.1099 - val_accuracy: 0.6688 - val_loss: 0.1099\n",
      "Epoch 5/10\n",
      "\u001b[1m18997/18997\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m414s\u001b[0m 22ms/step - accuracy: 0.6688 - loss: 0.1099 - val_accuracy: 0.6688 - val_loss: 0.1099\n",
      "Epoch 6/10\n",
      "\u001b[1m18997/18997\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m414s\u001b[0m 22ms/step - accuracy: 0.6689 - loss: 0.1099 - val_accuracy: 0.6688 - val_loss: 0.1099\n",
      "Epoch 7/10\n",
      "\u001b[1m18997/18997\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m414s\u001b[0m 22ms/step - accuracy: 0.6688 - loss: 0.1099 - val_accuracy: 0.6688 - val_loss: 0.1099\n",
      "Epoch 8/10\n",
      "\u001b[1m18997/18997\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m413s\u001b[0m 22ms/step - accuracy: 0.6688 - loss: 0.1099 - val_accuracy: 0.6688 - val_loss: 0.1098\n",
      "Epoch 9/10\n",
      "\u001b[1m18997/18997\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m413s\u001b[0m 22ms/step - accuracy: 0.6687 - loss: 0.1099 - val_accuracy: 0.6688 - val_loss: 0.1098\n",
      "Epoch 10/10\n",
      "\u001b[1m18997/18997\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m413s\u001b[0m 22ms/step - accuracy: 0.6688 - loss: 0.1098 - val_accuracy: 0.6688 - val_loss: 0.1098\n",
      "2024-04-20 22:23:37,892 — __main__ — save_model:175 — Saving model to './models/2f50e09c-e5fa-4c13-8592-74a2c816937e/model.h5'\n",
      "2024-04-20 22:23:37,893 — __main__ — save_model:176 — Creating directory './models/2f50e09c-e5fa-4c13-8592-74a2c816937e'\n",
      "2024-04-20 22:23:37,896 — __main__ — save_model:191 — Successfully created the directory ./models/2f50e09c-e5fa-4c13-8592-74a2c816937e\n",
      "2024-04-20 22:23:37,929 — __main__ — save_model:194 — Saved model weights to ./models/2f50e09c-e5fa-4c13-8592-74a2c816937e/model.weights.h5\n",
      "2024-04-20 22:23:37,930 — __main__ — save_model:196 — Saving Numpy random state to ./models/2f50e09c-e5fa-4c13-8592-74a2c816937e/numpy_random_state.pkl\n",
      "2024-04-20 22:23:37,933 — __main__ — save_model:199 — Saved Numpy random state to ./models/2f50e09c-e5fa-4c13-8592-74a2c816937e/random_state.npy\n",
      "2024-04-20 22:23:37,935 — __main__ — save_model:202 — Saving Network with UUID 2f50e09c-e5fa-4c13-8592-74a2c816937e\n",
      "{'seed': 42, 'uuid': '2f50e09c-e5fa-4c13-8592-74a2c816937e'}\n",
      "2024-04-20 22:23:37,936 — __main__ — save_model:205 — Saved state to './models/2f50e09c-e5fa-4c13-8592-74a2c816937e/state.json'\n",
      "\u001b[1m    9/18997\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:01\u001b[0m 6ms/step - accuracy: 0.6684 - loss: 0.1099  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-20 22:23:41.081280: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 10455948800 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18997/18997\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 6ms/step - accuracy: 0.6688 - loss: 0.1098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-20 22:25:49.990259: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 10455948800 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18997/18997\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 6ms/step\n",
      "2024-04-20 22:28:00,752 — __main__ — <module>:305 — Training complete\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "__author__ = [\"Jason Gardner\"]\n",
    "__credits__ = [\"Jason Gardner\"]\n",
    "__license__ = \"MIT\"\n",
    "__version__ = \"0.0.1\"\n",
    "__maintainer__ = [\"Jason Gardner\"]\n",
    "__email__ = [\"n01480000@unf.edu\"]\n",
    "__status__ = \"Development\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import uuid\n",
    "import pickle\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "import gc\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "EPOCHS = 10\n",
    "METRICS = [\"accuracy\"]\n",
    "BATCH_SIZE = 32\n",
    "SEQUENCE_SIZE = 100\n",
    "TEST_RATIO= 0.2\n",
    "LOG_FORMAT_STRING = logging.Formatter(\"%(asctime)s — %(name)s — %(funcName)s:%(lineno)d — %(message)s\")\n",
    "RMSPROP_CLIP = 10.0\n",
    "FILENAME = \"/data/NF-UQ-NIDS-v2.csv\"\n",
    "TEST_FILENAME = \"/data/NF-UQ-NIDS-v2_truncated.csv\"\n",
    "INPUT_SHAPE = (SEQUENCE_SIZE, 43)\n",
    "\n",
    "np.random.seed(SEED)\n",
    "\n",
    "class Data:\n",
    "    def __init__(self, test: bool = False) -> None:\n",
    "        self.test = test\n",
    "        self.data = self.load_data()\n",
    "        self.train_x, self.test_x, self.train_y, self.test_y = self._process_data()\n",
    "        \n",
    "    def load_data(self) -> pd.DataFrame:\n",
    "        if self.test:\n",
    "            logger.info(f\"Loading data from {TEST_FILENAME}\")\n",
    "            data = pd.read_csv(TEST_FILENAME)\n",
    "        else:\n",
    "            logger.info(f\"Loading data from {FILENAME}\")\n",
    "            data = pd.read_csv(FILENAME)\n",
    "        \n",
    "        return data\n",
    "\n",
    "    def _process_data(self) -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "        logger.info(\"Processing data\")\n",
    "        logger.info(\"Converting IP addresses to integers\")\n",
    "        self.data[\"IPV4_SRC_ADDR\"] = self.data[\"IPV4_SRC_ADDR\"].apply(lambda x: int(x.replace(\".\", \"\")))\n",
    "        self.data[\"IPV4_DST_ADDR\"] = self.data[\"IPV4_DST_ADDR\"].apply(lambda x: int(x.replace(\".\", \"\")))\n",
    "        logger.info(\"Dropping Attack and Dataset columns\")\n",
    "        self.data.drop(\"Attack\", axis = 1, inplace = True)\n",
    "        self.data.drop(\"Dataset\", axis = 1, inplace = True)\n",
    "        train_x, test_x, train_y, test_y = self._split_data(self.data)\n",
    "        logger.info(f\"Data dimensions: {self.data.shape}\")\n",
    "        logger.info(f\"Training data dimensions: {train_x.shape}\")\n",
    "        logger.info(f\"Testing data dimensions: {test_x.shape}\")\n",
    "        \n",
    "        return train_x, test_x, train_y, test_y\n",
    "    \n",
    "    def _split_data(self, data) -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "        train_size = int(len(data) * (1 - TEST_RATIO))\n",
    "        training_data, testing_data = data[:train_size], data[train_size:]\n",
    " \n",
    "        train_x = training_data.drop(\"Label\", axis=1).copy()\n",
    "        train_y = training_data[\"Label\"].copy()\n",
    "\n",
    "        test_x = testing_data.drop(\"Label\", axis=1).copy()\n",
    "        test_y = testing_data[\"Label\"].copy()\n",
    "        \n",
    "        self._normalize_data(train_x)\n",
    "        self._normalize_data(test_x)\n",
    "        \n",
    "        train_x = self._pad_data(np.array(train_x), 43)\n",
    "        train_x = np.array(train_x).reshape(-1, SEQUENCE_SIZE, 43)\n",
    "        train_y = self._pad_data(np.array(train_y))\n",
    "        train_y = np.array(train_y).reshape(-1, SEQUENCE_SIZE, 1)\n",
    "        test_x = self._pad_data(np.array(train_x), 43)\n",
    "        test_x = np.array(train_x).reshape(-1, SEQUENCE_SIZE, 43)\n",
    "        test_y = self._pad_data(np.array(train_y))\n",
    "        test_y = np.array(train_y).reshape(-1, SEQUENCE_SIZE, 1)\n",
    "        \n",
    "        logger.info(f\"Reshaped train_x shape: {train_x.shape}\")\n",
    "        logger.info(f\"Reshaped train_y shape: {train_y.shape}\")\n",
    "\n",
    "        return train_x, test_x, train_y, test_y\n",
    "    \n",
    "    def _pad_data(self, data, num_features = None):\n",
    "        if data.ndim == 1:\n",
    "            total_rows = data.shape[0]\n",
    "            rows_needed = SEQUENCE_SIZE - (total_rows % SEQUENCE_SIZE) if total_rows % SEQUENCE_SIZE != 0 else 0\n",
    "            \n",
    "            if rows_needed > 0:\n",
    "                padding = np.zeros(rows_needed, dtype=data.dtype)\n",
    "                data = np.concatenate([data, padding], axis=0)\n",
    "        elif data.ndim == 2:\n",
    "            total_rows = data.shape[0]\n",
    "            rows_needed = SEQUENCE_SIZE - (total_rows % SEQUENCE_SIZE) if total_rows % SEQUENCE_SIZE != 0 else 0\n",
    "            \n",
    "            if rows_needed > 0:\n",
    "                padding = np.zeros((rows_needed, num_features), dtype=data.dtype)\n",
    "                data = np.concatenate([data, padding], axis=0)\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def _normalize_data(self, data) -> pd.DataFrame:\n",
    "        logger.info(\"Normalizing data\")\n",
    "        for column in data.columns:\n",
    "            mean = data[column].mean()\n",
    "            std = data[column].std()\n",
    "            if column != \"Label\":\n",
    "                data[column] = (data[column] - mean) / std\n",
    "        \n",
    "        return data\n",
    "\n",
    "    def get_data(self) -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "        \n",
    "        return self.train_x, self.test_x, self.train_y, self.test_y\n",
    "    \n",
    "###############\n",
    "###  MODEL  ###\n",
    "###############\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, state : dict = None, size: int = None) -> None:\n",
    "        self.state = state\n",
    "        self.size = size\n",
    "        self.uuid = self.state['uuid']\n",
    "\n",
    "        if self.state['uuid'] is not None:\n",
    "            print(f\"Loading model from {self.state['uuid']}\")\n",
    "            self.load_model()\n",
    "        else:\n",
    "            logger.info(\"No model UUID provided. Generating new UUID\")\n",
    "            self.state['uuid'] = str(uuid.uuid4())\n",
    "            self.uuid = self.state['uuid']\n",
    "            logger.info(f\"New UUID: {self.uuid}\")\n",
    "            logger.info(f\"Initializing new model\")\n",
    "            self.init_model()\n",
    "\n",
    "    def init_model(self) -> tf.keras.Model:\n",
    "        logger.info(\"Initializing model\")\n",
    "        self.model = tf.keras.models.Sequential([\n",
    "                        layers.Input(shape=INPUT_SHAPE),\n",
    "                        layers.Conv1D(filters=32, kernel_size=3, strides=1, activation=\"relu\", padding=\"same\"),\n",
    "                        layers.MaxPooling1D(pool_size=2, padding=\"same\"),\n",
    "                        layers.Conv1D(filters=64, kernel_size=3, strides=1, activation=\"relu\", padding=\"same\"),\n",
    "                        layers.MaxPooling1D(pool_size=2, padding=\"same\"),\n",
    "                        layers.Conv1D(filters=128, kernel_size=3, strides=1, activation=\"relu\", padding=\"same\"),\n",
    "                        layers.MaxPooling1D(pool_size=2, padding=\"same\"),\n",
    "                        layers.LSTM(128, activation=\"relu\", return_sequences=True),\n",
    "                        layers.LSTM(64, activation=\"relu\"),\n",
    "                        layers.Dense(1, activation=\"sigmoid\")\n",
    "                    ])\n",
    "      \n",
    "        self.optimizer = tf.keras.optimizers.RMSprop(clipvalue = RMSPROP_CLIP)\n",
    "        # self.optimizer = tf.keras.optimizers.Adam(lr=1e-3)\n",
    "        self.loss_function = tf.keras.losses.Huber()\n",
    "        # self.loss_function = tf.keras.losses.mean_squared_error\n",
    "        self.model.compile(optimizer = self.optimizer, loss = self.loss_function, metrics = METRICS)\n",
    "        logger.info(\"Model initialized\")\n",
    "           \n",
    "    def save_model(self) -> None:\n",
    "        logger.info(f\"Saving model to './models/{self.uuid}/model.h5'\")\n",
    "        logger.info(f\"Creating directory './models/{self.state['uuid']}'\")\n",
    "        results_dir_path = f\"./models/{self.state['uuid']}\"\n",
    "        if not os.path.exists(results_dir_path):\n",
    "            if not os.path.exists('./models'):\n",
    "                try:\n",
    "                    os.mkdir('./models')\n",
    "                except OSError:\n",
    "                    logger.warning(f\"Creation of the directory {'./models'} failed\")\n",
    "                    exit(1)\n",
    "            try:\n",
    "                os.mkdir(results_dir_path)\n",
    "            except OSError:\n",
    "                logger.warning(f\"Creation of the directory {results_dir_path} failed\")\n",
    "                exit(1)\n",
    "            else:\n",
    "                logger.info(f\"Successfully created the directory {results_dir_path}\")\n",
    "\n",
    "        self.model.save_weights(f'./models/{self.state[\"uuid\"]}/model.weights.h5')\n",
    "        logger.info(f\"Saved model weights to ./models/{self.state['uuid']}/model.weights.h5\")\n",
    "\n",
    "        logger.info(f\"Saving Numpy random state to ./models/{self.state['uuid']}/numpy_random_state.pkl\")\n",
    "        with open(f\"./models/{self.state['uuid']}/numpy_random_state.pkl\", 'wb') as f:\n",
    "            pickle.dump(np.random.get_state(), f)\n",
    "        logger.info(f\"Saved Numpy random state to ./models/{self.state['uuid']}/random_state.npy\")\n",
    "\n",
    "        with open(f'./models/{self.state[\"uuid\"]}/state.json', 'w') as f:\n",
    "            logger.info(f\"Saving Network with UUID {self.state['uuid']}\")\n",
    "            print(self.state)\n",
    "            json.dump(self.state, f)\n",
    "            logger.info(f\"Saved state to './models/{self.uuid}/state.json'\")\n",
    "\n",
    "    def load_model(self) -> tf.keras.Model:\n",
    "        self.model = tf.keras.Model()\n",
    "        logger.info(f\"Loading model from './models/{self.uuid}/model.weights.h5'\")\n",
    "        self.model.load_weights(f'./models/{self.uuid}/model.weights.h5')\n",
    "        logger.info(f\"Loaded model weights from './models/{self.uuid}/model.weights.h5'\")\n",
    "        \n",
    "        with open(f\"./models/{self.state['uuid']}/numpy_random_state.pkl\", 'rb') as f:\n",
    "            random_state = pickle.load(f)\n",
    "            np.random.set_state(random_state)\n",
    "        logger.info(f\"Loaded Numpy random state from './models/{self.uuid}/numpy_random_state.pkl'\")\n",
    "\n",
    "        if os.path.exists(f'./models/{self.uuid}/state.json'):\n",
    "            with open(f'./models/{self.uuid}/state.json', 'r') as f:\n",
    "                logger.info(f\"Loading state from './models/{self.uuid}/state.json'\")\n",
    "                self.state = json.load(f)\n",
    "                logger.info(f\"Loading {self.state['network']} ({self.state['uuid']}) network environment.\\n Game: {self.state['environment']}\")\n",
    "                logger.info(f\"Loaded state from './models/{self.uuid}/state.json'\")     \n",
    "\n",
    "        return self.model\n",
    "\n",
    "###############\n",
    "### LOGGING ###\n",
    "###############\n",
    "         \n",
    "class Logging:\n",
    "    def __init__(self, logger_name: str = '__main__') -> None:\n",
    "        self.logger = logging.getLogger(logger_name)\n",
    "        self.logger.setLevel(logging.DEBUG)\n",
    "        self.logger.addHandler(self.get_console_handler())\n",
    "        self.logger.propagate = False\n",
    "        self.logger.info(f\"Logging initialized -- {logger_name}\")\n",
    "\n",
    "    def get_console_handler(self) -> logging.StreamHandler:\n",
    "        console_handler = logging.StreamHandler(sys.stdout)\n",
    "        console_handler.setFormatter(LOG_FORMAT_STRING)\n",
    "\n",
    "        return console_handler\n",
    "\n",
    "    def get_logger(self) -> logging.Logger:\n",
    "\n",
    "        return self.logger\n",
    "      \n",
    "###############\n",
    "###  AGENT  ###\n",
    "###############\n",
    "      \n",
    "class Agent:\n",
    "    def __init__(self, state: dict, data : tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]) -> None:\n",
    "        self.uuid = state['uuid']\n",
    "        self.optimizer = tf.keras.optimizers.RMSprop(clipvalue = RMSPROP_CLIP)\n",
    "        self.loss_function = tf.keras.losses.Huber()\n",
    "        self.train_x, self.test_x, self.train_y, self.test_y = data\n",
    "        self.M = Model(state = parser.data, size = len(self.train_y))\n",
    "        self.model = self.M.model\n",
    "        \n",
    "    def run(self) -> None:\n",
    "        print(f\"Data: {self.train_x.shape}\")\n",
    "        earlystopping = callbacks.EarlyStopping(monitor = \"val_loss\",\n",
    "                                        mode = \"min\",\n",
    "                                        patience = 5,\n",
    "                                        restore_best_weights = True)\n",
    "        history = self.model.fit(self.train_x, \n",
    "                       self.train_y, \n",
    "                       epochs = EPOCHS, \n",
    "                       batch_size = BATCH_SIZE, \n",
    "                       verbose = 1, \n",
    "                       validation_data = (self.test_x, self.test_y)\n",
    "                       , callbacks = [earlystopping])\n",
    "        self.M.save_model()\n",
    "        self.model.evaluate(self.test_x, self.test_y)\n",
    "        self.model.predict(self.test_x)\n",
    "        hist_df = pd.DataFrame(history.history) \n",
    "\n",
    "        hist_csv_file = f\"./models/{parser.data['uuid']}/history.csv\"\n",
    "        with open(hist_csv_file, mode='w') as f:\n",
    "            hist_df.to_csv(f)\n",
    "\n",
    "##########################\n",
    "###  ARGUMENT PARSING  ###\n",
    "##########################\n",
    "\n",
    "class Parsing:\n",
    "    def __init__(self):\n",
    "        self.data = dict()\n",
    "        self.data['seed'] = 42\n",
    "        self.data['uuid'] = None\n",
    "\n",
    "###############\n",
    "###  MAIN   ###\n",
    "###############\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    gc.collect()\n",
    "    logger = Logging().get_logger()\n",
    "    parser = Parsing()\n",
    "    data = Data().get_data()\n",
    "    agent = Agent(parser.data, data)\n",
    "    agent.run()\n",
    "    logger.info(\"Training complete\")\n",
    "    exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1f5c6d-2582-4cd3-87a8-e403abcd6cf9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
